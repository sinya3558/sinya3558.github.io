---
layout: single
title:  "SGD vs SGD w.ëª¨ë©˜í…€ ë¹„êµ"
description: "ipynb íŒŒì¼ì„ .md ë¡œ ë³€í™˜ì‹œì¼œ ì˜¬ë ¤ë³´ëŠ” í…ŒìŠ¤íŠ¸ ê¸€"
date: 2025-10-31
categories: [Deep Learning]
tag: [SGD, Momentum, Optimization, Gradient Descent]
toc: true
author_profile: True
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>


# (1) ê²½ì‚¬í•˜ê°•ë²•(SGD) ê³¼ (2) ëª¨ë©˜í…€ì„ ì ìš©í•œ SGD ë¹„êµ



ê¸°ë³¸ ê²½ì‚¬í•˜ê°•ë²•(Stochastic Gradient Descent, SGD)ê³¼ **ëª¨ë©˜í…€(Momentum)** ì„ ì ìš©í•œ ê²½ì‚¬í•˜ê°•ë²•ì˜ ì°¨ì´ë¥¼ PyTorchë¡œ ë¹„êµ



## (1) ê¸°ë³¸ SGD



- Pytorch ì˜ 'optim' ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•´ì„œ ìë™ìœ¼ë¡œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ê³„ì‚° ë° ì—…ë°ì´íŠ¸

- í…ì„œ'torch.tensor'ì™€ ë„˜íŒŒì´'numpy.array()' ê°„ ë³€í™˜ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„ í•  ê²ƒ



```python
import numpy as np
import torch
import matplotlib.pyplot as plt
from torch import optim
```


```python
# ìƒ˜í”Œ ë°ì´í„° ì„ ì–¸
sampleData1 = np.array([
    [166, 58.7],
    [176, 75.7], 
    [171, 62.1],
    [173, 70.4],
    [169, 60.1]
])

# cost function : MSE
def mse(Yp, Y):
    loss = ((Yp - Y)**2).mean()
    return loss

# x, y ë§Œë“¤ê¸°
x = sampleData1[:, 0]   # í‚¤
y = sampleData1[:, 1]   # ëª¸ë¬´ê²Œ

# x, y ê°’ì„ í‰ê·  ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ (loss ê°’ NaN ë°©ì§€âœ¨)
X = (x - x.mean())
Y = (y - y.mean())

# x,y ë³€ìˆ˜ -> í…ì„œë¡œ ë³€í™˜
X = torch.tensor(X)
Y = torch.tensor(Y)
# print(x, y)
# print(X, Y)
```


```python
# ìµœì í™” í•¨ìˆ˜ ì‚¬ìš©í•˜ê¸°
# from torch import optim # GD, SGD, SGD with momentum, Adam, ... ì „ë¶€ optim ì•ˆì— ìˆëŠ” ìµœì í™”ë¥¼ ìœ„í•œ í•¨ìˆ˜! 
'''
- ìµœì í™” í•¨ìˆ˜ë€?
with torch.no_grad():
    W -= lr* W.grad
    B -= lr* B.grad

ì´ê±¸ ìë™ìœ¼ë¡œ ëŒ€ì‹ í•´ì¤Œ
'''

# 1. ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ì´ˆê¸°ê°’ ì„¤ì •
W = torch.tensor(1., requires_grad = True)
B = torch.tensor(1., requires_grad = True)

# ì˜ˆì¸¡ í•¨ìˆ˜ ë° ì˜ˆì¸¡ê°’
def pred(X):
    y = W*X +B
    return y

# 2. hyperparmeter ì„¤ì •(lr, # epochs)
lr = 0.001
num_epochs = 500

optimizer = optim.SGD([W, B], lr= lr)

# 3. ê°’ ì €ì¥í•˜ëŠ” ë…€ì„ ë§Œë“¤ê¸°
history = np.zeros([0,2])   # row : 0 ê°œ, col: ë‘ ê°œ (epoch, loss(MSE))
```


```python
# 4. í•™ìŠµ
for epoch in range(num_epochs):

    ## ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ê³„ì‚°
    Yp = pred(X)

    ## loss êµ¬í•˜ê¸°
    loss = mse(Yp, Y)  # í…ì„œêµ¬ì¡°

    ## gradient ê³„ì‚°
    loss.backward()

    ## ê³„ì‚°ëœ í•™ìŠµ íŒŒë¼ë¯¸í„° ìë™ ì—…ë°ì´íŠ¸
    optimizer.step()

    ## W, B ì´ˆê¸°í™”
    optimizer.zero_grad()

    # 0, 10, 20, 30,,... ì—í­ë•Œ ì €ì¥í•´ë¼
    if epoch % 10 == 0:
        item = np.array([epoch, loss.item()])  # í…ì„œì—ì„œ ë„˜íŒŒì´ë¡œ ì ‘ê·¼
        history = np.vstack([history, item])   # ì•„ì´í…œ ë‚´ìš© history ì•ˆì— ì €ì¥
        print(f"Epoch: {epoch + 1}, Loss: {loss.item():.3f}")
```

```python
# loss ì‹œê°í™”
plt.plot(history[:, 0], history[:, 1], label= "SGD")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()
```

![SGD_Loss](/assets/img/posts/sgd_loss.png)


## (2) SGD with Momentum



- ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼í•˜ê³  optimizer ì„¤ì •ë§Œ `momentum = 0.9` ì¶”ê°€í•˜ì—¬ ë‹¤ë¦„

- "ê°€ì†ë„(momentum)"ì˜ ê°œë…ì„ ë„ì…í•´, ê²½ì‚¬ê°€ ì‘ì€ êµ¬ê°„ì—ì„œë„ ê¾¸ì¤€íˆ ì´ë™í•  ìˆ˜ ìˆë„ë¡ ë„ì›€

- ì¼ë°˜ SGD ì™€ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ í™•ì¸



```python
# 1. W, B ì´ˆê¸°ê°’ ì„¤ì •
W = torch.tensor(1.0, requires_grad = True)
B = torch.tensor(1.0, requires_grad = True)

# 2. ë™ì¼í•œ í•™ìŠµë¥ ê³¼ ì—í­
lr = 0.001
num_epochs = 500

#ğŸ”¸ 2.5 ìµœì í™”í•¨ìˆ˜(momentum ì¶”ê°€)
optimizer = optim.SGD([W, B], lr= lr, momentum = 0.9)

# 3. ì†ì‹¤ ì¶”ì²™í•˜ëŠ” ë…€ì„ ë§Œë“¤ê¸°
# history for SGD w momentum
history2 = np.zeros([0, 2]) # epoch, loss
```


```python
# 4. í•™ìŠµ
for epoch in range(num_epochs):
    Yp = pred(X)
    loss = mse(Yp, Y)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    if epoch % 10 == 0:
        item = np.array([epoch, loss.item()])  
        history2 = np.vstack([history2, item])
        print(f"Epoch: {epoch + 1}, Loss: {loss.item():.4f}")
```

```python
# ì†ì‹¤ ë¹„êµ ì‹œê°í™”(SGD ì™€ SGD w. momentum)
plt.plot(history[:, 0], history[:, 1], label = "SGD")
plt.plot(history2[:, 0], history2[:, 1], label = "SGD w Momentum 0.9")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()
```
![SGD_Loss_w_Momemtum](/assets/img/posts/sgd_loss_momentum.png)
