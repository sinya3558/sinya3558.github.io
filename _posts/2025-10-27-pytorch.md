---
title: "PyTorch 기본 개념 정리"
description: "Tensor 구조와 주요 연산을 중심으로 살펴보는 PyTorch 기초 노트"
author: Seunga Kim
date: 2025-10-27
categories: [Deep Learning]
tags: [PyTorch, Tensor, AI, Python]
toc: true
toc_label: "Table of Contents"
comments: false
---

##### 📚 참고 자료
- 🔗[Pytorch](https://tutorials.pytorch.kr/beginner/basics/tensorqs_tutorial.html)
- 🔗[Tensor Basics in PyTorch](https://medium.com/codex/tensor-basics-in-pytorch-252a34288f2)
- 🔗 두산 로키 6기 강의자료


## PyTorch란?

> - 페이스북이 개발한 딥러닝 신경망 구축을 위한 오픈소스 프레임워크  
> - Torch 머신러닝 라이브러리 + Python API 를 결합  
> - **동적 연산 그래프** 기반으로 디버깅과 실험이 쉬움 → TF 보다 학습 용이  
> - Hugging Face, GitHub 에서 활발하게 사용됨  
> - **자동 미분 기능**은 파이토치의 최대 특징  
> - GPU 연산 지원으로 빠른 학습이 가능  


---

## 1. PyTorch의 기본 구조 자료형, Tensor

### Tensor
- PyTorch의 핵심 데이터 표현 구조  
- NumPy 배열과 유사하나 GPU 가속 가능 


![Tensor example](/assets/img/tensor.png)  
[그림 출처](https://medium.com/codex/tensor-basics-in-pytorch-252a34288f2)


#### 텐서의 종류
- Scalar : 0 차원 텐서  
- Vector : 1차원 텐서  
- Matrix : 2차원 텐서  
- Tensor : 3차원 이상 (보통 3~5차원까지 사용)

#### 텐서의 자료형 예시
- `torch.FloatTensor` : 32-bit 실수형 텐서  
- `torch.IntTensor` : 32-bit 정수형 텐서  
- `torch.cuda.FloatTensor` : GPU 가속된 실수형 텐서, cuda device 에서 수행되는 32 비트 실수형 텐서  

> **💡 PyTorch가 32 비트를 디폴트 값으로 사용하는 이유**
>
> 비트 수는 하나의 수를 표현하는데 필요한 메모리 용량.
> **커질수록** 표현 가능한 숫자의 범위와 정밀도가 커지나, 메모리 사용량과 연산 시간이 늘어남… 
>
> 1. `float32` (약 7자리) → GPU 연산의 기본 포맷  
>    - 속도와 정확도 균형이 좋고, 대부분의 CUDA GPU 는 float32 연산에 최적화되어 있음  
> 2. `float64` → 정밀하지만 속도가 2~4배 느림, GPU 자원 소모 큼  
> 3. `float16` → 빠르지만 정밀도 낮아 (half precision) overflow 가능  
>
>    → 상황에 따라 1~3 선택해서 사용  

---

#### 텐서 적용 코드 예시

```python
import torch

# Scalar (0차원 텐서)
dim_0 = torch.tensor(5)

# Vector (1차원 텐서)
dim_1 = torch.tensor([1, 2, 3])

# Matrix (2차원 텐서)
dim_2 = torch.tensor([[4, 5, 6], [7, 8, 9]])

# Tensor (3차원 텐서: batch x row x col)
dim_3 = torch.randn(2, 3, 4)  # randn(): randomly normal != normalized 아님
```

> 딥러닝에서는 보통 모델의 가중치를 랜덤으로 초기화시켜
> torch.randn()으로 평균 0, 표준편차 1의 정규분포에서 랜덤 값을 생성한다.

<details>
  <summary>📌 (구분할 것) <b>randomly normal vs randomly normalized</b></summary>

<br>

#### randomly normal
- 무작위로 정규 분포에서 뽑은 값  
- 요소 모두 **평균이 0**, **표준편차가 1인 (+- 1)** 정규분포에서 랜덤으로 뽑은 값들  
- 코드 예제  

    ```python
    normal_x = torch.randn(5)
    # norma_x = [-0.21, 0.45, 1.03, -0.78, 0.05]
    # → 5개 요소 모두 평균이 0, 표준편차가 1인 (+- 1) 정규분포에서 랜덤으로 뽑은 값들
    ```


#### randomly normalized

- 무작위 값을 정규화(normalize)한 것  
  - 보통 랜덤으로 값을 뽑은뒤, 정규화 해줘서 특정 범위나 크기로 맞추는 거  
  - 코드 예제  

    ```python
    import torch

    normal_x = torch.randn(5)
    # normal_x  =  [-0.21,  0.45,  1.03, -0.78,  0.05] 이라고 했을 때,

    x_normalized =  normal_x / torch.norm(normal_x)
    # x_normalized = [-0.1515,  0.3250,  0.7440, -0.5630,  0.0361]
    # 방향은 같으나 벡터 크기가 1로 정규화된-normalized- 상태
    ```

</details>

--- 

## 2. 텐서 사용을 위한 중요한 함수 3가지

### 2-1. view()

> NumPy의 `reshape()`처럼 텐서의 차원 혹은 계수(rank)를 변환하는 함수

- `tensor.view(shape)` : 텐서의 모양을 재배열  
    - shape: 바꾸고 싶은 새로운 차원 구조 (rank)
- **📍주의:** 원소의 총 개수(`numel`)는 동일해야 함  
- 기존 텐서의 데이터는 그대로 두고 **모양(shape)** 만 바꾸는 함수 
- `tensor.view(-1)` : 전체를 1차원으로 변경
    - n차원 형태여도, **-1** 로 넘겨지는 부분의 요소 수는 자동으로 총 개수에 맞춰 조정됨
- 코드 예제

    ```python
    import torch

    x = torch.arange(12)
    print(x.shape)     # torch.Size([12])

    # 2x6 로 변경
    y = x.view(2, 6)
    print(y.shape)     # torch.Size([2, 6])

    # 2x6 텐서를 3x4 텐서로 변환
    y = x.view(3, 4)

    # 3차원 형태로도 변경 가능
    z = x.view(2, 3, 2)
    print(z.shape)     # torch.Size([2, 3, 2])

    # 다시 1차원 형태로 변경 가능
    q = z.view(-1)
    print(q.shape)  # torch.Size([12])
    ```

### 2-2. item()
> 스칼라에 대해서는(0 차원 텐서), 텐서에서 파이썬 본래 클래스의 수를 꺼낼 때, item 함수를 사용

- 텐서로 이루어진 loss 계산 결과에서 데이터 기록을 위한 값을 추출하는데 자주 사용.
- 스칼라(0 차원 텐서) 이외의 텐서에 item  함수는 무효함
    - 단! shape 이 [1] 이거나 [1, 1] 처럼 1차원 혹은 2차원 텐서라도 **요소가 한 개밖에 없는 경우**는 사용이 가능하다
- 구현 코드 예제

    ```python
    # 1. item 함수 사용
    item = x.item()

    print(type(item)) # <class 'float'>
    print(item) # 1.0

    # 2. 요소가 하나뿐인 1 차원 텐서 item 사용해보기
    t1 = tordh.ones(1)

    print(t1.shape)   # torch.Size([1])
    print(t1.item())  # 1.0
    ```

### 2-3. max()
> 최대치를 가져오는 max 함수 ⇒ 모델의 예측 값 중 가장 확률이 높은 값을 뽑을 때 max 함수 사용

- 구현 코드 예제

    ```python
    # 1. max 함수 인수없이 호출하면, 최댓값을 얻음
    r1 = [[1, 5, 6],
                [4, 3, 2]]

    print(r1.max())   # tensor(6) : 6 개 요소 중 최댓값 반환
    ```

- min(), mean() 함수도 같은 방식으로 사용



- torch.max 함수를 호출 하는 방법도 있다
    - 2D 텐서의 경우, 축 = 1 은 row 방향, 축 = 0 은 col 방향을 의미한다 

        ```python
        # 2. torch.max() -> 두 번째 인수는 기준이 되는 축(axis)을 의미함
        print(torch.max(r1, 1))

        '''
        torch.return_types.max( values = tensor([6, 4]), indices = tensor([2, 0])
        '''

        ```

        - 최댓값 뿐 아니라, 어떤 index 에서 최댓값을 가져왔는지 indices 도 같이 반환한다
            - 즉, 최대값 6 → index = 2 (세 번째)
            - 최대값 4 → index = 0 ( 첫 번째)
        - 다중 분류에서 예측 라벨을 구할 때 자주 사용되는 패턴이다
            - `print(torch.max(r1, 1)[1])`
            -  “여러 개의 예측 출력 중, 가장 큰 값을 낸 예측의 index 를 예측 결과의 라벨로 뽑아와라”
---

## 3. 연산을 위한 텐서 변환 함수 2가지
### 3-1. unsqueeze()

- 지정된 위치 ( dim = index) 에 **새로운 차원을 추가**
    - 0 이면 맨 앞에 추가
    - -1 이면 맨 뒤에 추가

**자주 쓰이는 경우**

1. 배치 차원 추가

    ```python
    img = torch.randn(3, 224, 224)  # (C, H, W)
    img = img.unsqueeze(0)           # (1, C, H, W)  → 배치 1개
    ```

2. 브로드캐스팅 차원 맞추기

    ```python
    x = torch.randn(5)
    y = torch.randn(5, 1)
    x = x.unsqueeze(1)  # (5, 1) → y와 브로드캐스트 가능
    ```

**예제 표**

| 텐서 | unsqueeze() 연산   | 변환 후 텐서   |
| -------- | -------------- | ----------- |
| `[4]`    | `unsqueeze(0)` | `[1, 4]`    |
| `[4]`    | `unsqueeze(1)` | `[4, 1]`    |
| `[2, 3]` | `unsqueeze(0)` | `[1, 2, 3]` |
| `[2, 3]` | `unsqueeze(2)` | `[2, 3, 1]` |


### 3-2. squeeze()

- 데이터는 그대로 두고 **불필요한 차원**만 줄이는 것
- 크기가 1 인 차원을 모두 제거 → 텐서의 shape 중에서 `(1,)`인 차원들
    - 단, 옵션을 사용하면 <u>특정 축의 차원</u>만 제거 가능
        - dim (int, **optional**): 제거할 차원 위치 지정

            ```python
            tensor.squeeze(dim=None)
            ```

            - <u>지정하지 않으면</u> 모든 크기 1짜리 차원을 제거
            - 지정하면 해당 차원이 **1일 때만 제거**됨

**자주 쓰이는 경우**

1. 모델 출력에서 배치 차원 제거할 때

    ```python
    pred = torch.randn(1, 10)  # (1, 10)
    pred = pred.squeeze(0)     # (10,)
    ```

2. 브로드 캐스팅 후 결과 정리

    ```python
    y = torch.randn(3, 1, 1)
    y = y.squeeze()            # (3,)
    ```


**예제 표**

| 텐서       | squeeze() 적용            | 변환 후 텐서   |
|------------------|----------------|----------------|
| `[1, 3, 1, 5]`   | `squeeze()`    | `[3, 5]`       |
| `[1, 3, 1, 5]`   | `squeeze(0)`   | `[3, 1, 5]`    |
| `[1, 3, 1, 5]`   | `squeeze(2)`   | `[1, 3, 5]`    |

<br>

### 3-3. squeeze vs unsqueeze 텐서 변환 구조 🍃

![PyTorch Tensor 구조](/assets/img/20251027_121458_SamsungNotes.jpg)
